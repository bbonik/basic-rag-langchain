{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation: Question Answering based on Custom Dataset with Open-sourced [LangChain](https://python.langchain.com/en/latest/index.html) Library\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!!This notebook should be run using the Data Science 2.0 Python kernel!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will demonstrate how to use a **Falcon 7B FP16** to answer questions using a library of documents as a reference, by using document embeddings and retrieval. The embeddings are generated from **GPT-J-6B-FP16** embedding model. \n",
    "\n",
    "**This notebook serves a template such that you can easily replace the example dataset by your own to build a custom question and asnwering application.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Deploy large language model (LLM) and embedding model in SageMaker JumpStart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: jsonschema 3.2.0 does not provide the extra 'format-nongpl'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker-datawrangler 0.4.3 requires sagemaker-data-insights==0.4.0, but you have sagemaker-data-insights 0.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade sagemaker --quiet\n",
    "!pip install ipywidgets==7.0.0 --quiet\n",
    "!pip install langchain==0.0.148 --quiet\n",
    "!pip install faiss-cpu --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "from typing import Any, Dict, List, Optional\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.llms.sagemaker_endpoint import ContentHandlerBase\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()\n",
    "model_version = \"*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy SageMaker endpoint(s) for large language models and GPT-J 6B embedding model. Please uncomment the entries as below if you want to deploy multiple LLM models to compare their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information [here](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.jumpstart.model.JumpStartModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_endpoint_name = \"falcon-7b-instruct-bf16-endpoint\"\n",
    "embeddings_endpoint_name = \"gpt-j-6b-endpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!CPU times: user 275 ms, sys: 33.2 ms, total: 308 ms\n",
      "Wall time: 9min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "model_llm = JumpStartModel(\n",
    "    model_id=\"huggingface-llm-falcon-7b-instruct-bf16\",\n",
    "    instance_type=\"ml.g5.2xlarge\"\n",
    ")\n",
    "predictor_llm = model_llm.deploy(\n",
    "    endpoint_name=llm_endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!CPU times: user 118 ms, sys: 18.6 ms, total: 137 ms\n",
      "Wall time: 7min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_embedding = JumpStartModel(\n",
    "    model_id=\"huggingface-textembedding-gpt-j-6b-fp16\",\n",
    "    instance_type=\"ml.p3.2xlarge\"\n",
    ")\n",
    "predictor_embedding = model_embedding.deploy(\n",
    "    endpoint_name=embeddings_endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Ask a question to LLM without providing the context\n",
    "\n",
    "To better illustrate why we need retrieval-augmented generation (RAG) based approach to solve the question and anwering problem. Let's directly ask the model a question and see how they respond."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the SageMaker Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Managed Spot Training in SageMaker can be used for a variety of instances, including:\n",
      "\n",
      "1. Training a model on a large dataset\n",
      "2. Training a model on a small dataset\n",
      "3. Training a model on a dataset with a large number of classes\n",
      "4. Training a model on a dataset with a small number of classes\n",
      "5. Training a model on a dataset with a large number of classes\n",
      "6. Training a model on a dataset with a small number of classes\n",
      "7. Training a model on a dataset with a large number of classes\n",
      "8. Training a model on a dataset with a small number of classes\n",
      "9. Training a model on a dataset with a large number of classes\n",
      "10. Training a model on a dataset with a small number of classes\n",
      "11. Training a model on a dataset with a large number of classes\n",
      "12. Training a model on a dataset with a small number of classes\n",
      "13. Training a model on a dataset with a large number of classes\n",
      "14. Training a model on a dataset with a small number of classes\n",
      "15. Training a model on a dataset with a large number of classes\n",
      "16. Training a model on a dataset with a small number of classes\n",
      "17. Training a model on a dataset with a large number of classes\n",
      "18. Training a model on a dataset with a small number of classes\n",
      "19. Training a model on a dataset with a large number of classes\n",
      "20. Training a model on a dataset with a small number of classes\n",
      "21. Training a model on a dataset with a large number of classes\n",
      "22. Training a model on a dataset with a small number of classes\n",
      "23. Training a model on a dataset with a large number of classes\n",
      "24. Training a model on a dataset with a small number of classes\n",
      "25. Training a model on a dataset with a large number of classes\n",
      "26. Training a model on a dataset with a small number of classes\n",
      "27. Training a model on a dataset with a large number of classes\n",
      "28. Training a model on a dataset with a small number of classes\n",
      "29. Training a model on a dataset with a large number of classes\n",
      "30. Training a model on a dataset with a small number of classes\n",
      "31. Training a model on a dataset with a large number of classes\n",
      "32. Training a model on a dataset with a small number of classes\n",
      "33. Training a model on a dataset with a large number of classes\n",
      "34. Training a model on a dataset with a small number of classes\n",
      "35. Training a model on a dataset with a large number of classes\n",
      "36. Training a model on a dataset with a small number of classes\n",
      "37. Training a model on a dataset with a large number of classes\n",
      "38. Training a model on a dataset with a small number of classes\n",
      "39. Training a model on a dataset with a large number of classes\n",
      "40. Training a model on a dataset with a small number of classes\n",
      "41. Training a model on a dataset with a large number of classes\n",
      "42. Training a model on a dataset with a small number of classes\n",
      "43. Training a model on a dataset with a large number of classes\n",
      "44. Training a model on a dataset with a small number of classes\n",
      "45. Training a model on a dataset with a large number of classes\n",
      "46. Training a model on a dataset with a small number of classes\n",
      "47. Training a model on a dataset with a large number of classes\n",
      "48. Training a model on a dataset with a small number of classes\n",
      "49. Training a model on a dataset with a large number of classes\n",
      "50. Training a model on a dataset with a small number of classes\n",
      "51. Training a model on a dataset with a large number of classes\n",
      "52. Training a model on a dataset with a small number of classes\n",
      "53. Training a model on a dataset with a large number of classes\n",
      "54. Training a model on a dataset with a small number of classes\n",
      "55. Training a model on a dataset with a large number of classes\n",
      "56. Training a model on a dataset with a small number of classes\n",
      "57. Training a model on a dataset with a large number of classes\n",
      "58. Training a model on a dataset with a small number of classes\n",
      "59. Training a model on a dataset with a large number of classes\n",
      "60. Training a model on a dataset with a small number of classes\n",
      "61. Training a model on a dataset with a large number of classes\n",
      "62. Training a model on a dataset with a small number of classes\n",
      "63. Training a model on a dataset with a large number of classes\n",
      "64. Training a model on a dataset with a small number of classes\n",
      "65. Training a model on a dataset with a large number of classes\n",
      "66. Training a model on a dataset with a small number of classes\n",
      "67. Training a model on a dataset with a large number of classes\n",
      "68. Training a model on a dataset with\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Which instances can I use with Managed Spot Training in SageMaker?\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.01,\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"stop\": [\"<|endoftext|>\", \"</s>\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "response = predictor_llm.predict(payload)\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Boto3 library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which instances can I use with Managed Spot Training in SageMaker?\n",
      "\n",
      "Managed Spot Training in SageMaker can be used for a variety of instances, including:\n",
      "\n",
      "1. Training a model on a large dataset\n",
      "2. Training a model on a small dataset\n",
      "3. Training a model on a dataset with a large number of classes\n",
      "4. Training a model on a dataset with a small number of classes\n",
      "5. Training a model on a dataset with a large number of classes\n",
      "6. Training a model on a dataset with a small number of classes\n",
      "7. Training a model on a dataset with a large number of classes\n",
      "8. Training a model on a dataset with a small number of classes\n",
      "9. Training a model on a dataset with a large number of classes\n",
      "10. Training a model on a dataset with a small number of classes\n",
      "11. Training a model on a dataset with a large number of classes\n",
      "12. Training a model on a dataset with a small number of classes\n",
      "13. Training a model on a dataset with a large number of classes\n",
      "14. Training a model on a dataset with a small number of classes\n",
      "15. Training a model on a dataset with a large number of classes\n",
      "16. Training a model on a dataset with a small number of classes\n",
      "17. Training a model on a dataset with a large number of classes\n",
      "18. Training a model on a dataset with a small number of classes\n",
      "19. Training a model on a dataset with a large number of classes\n",
      "20. Training a model on a dataset with a small number of classes\n",
      "21. Training a model on a dataset with a large number of classes\n",
      "22. Training a model on a dataset with a small number of classes\n",
      "23. Training a model on a dataset with a large number of classes\n",
      "24. Training a model on a dataset with a small number of classes\n",
      "25. Training a model on a dataset with a large number of classes\n",
      "26. Training a model on a dataset with a small number of classes\n",
      "27. Training a model on a dataset with a large number of classes\n",
      "28. Training a model on a dataset with a small number of classes\n",
      "29. Training a model on a dataset with a large number of classes\n",
      "30. Training a model on a dataset with a small number of classes\n",
      "31. Training a model on a dataset with a large number of classes\n",
      "32. Training a model on a dataset with a small number of classes\n",
      "33. Training a model on a dataset with a large number of classes\n",
      "34. Training a model on a dataset with a small number of classes\n",
      "35. Training a model on a dataset with a large number of classes\n",
      "36. Training a model on a dataset with a small number of classes\n",
      "37. Training a model on a dataset with a large number of classes\n",
      "38. Training a model on a dataset with a small number of classes\n",
      "39. Training a model on a dataset with a large number of classes\n",
      "40. Training a model on a dataset with a small number of classes\n",
      "41. Training a model on a dataset with a large number of classes\n",
      "42. Training a model on a dataset with a small number of classes\n",
      "43. Training a model on a dataset with a large number of classes\n",
      "44. Training a model on a dataset with a small number of classes\n",
      "45. Training a model on a dataset with a large number of classes\n",
      "46. Training a model on a dataset with a small number of classes\n",
      "47. Training a model on a dataset with a large number of classes\n",
      "48. Training a model on a dataset with a small number of classes\n",
      "49. Training a model on a dataset with a large number of classes\n",
      "50. Training a model on a dataset with a small number of classes\n",
      "51. Training a model on a dataset with a large number of classes\n",
      "52. Training a model on a dataset with a small number of classes\n",
      "53. Training a model on a dataset with a large number of classes\n",
      "54. Training a model on a dataset with a small number of classes\n",
      "55. Training a model on a dataset with a large number of classes\n",
      "56. Training a model on a dataset with a small number of classes\n",
      "57. Training a model on a dataset with a large number of classes\n",
      "58. Training a model on a dataset with a small number of classes\n",
      "59. Training a model on a dataset with a large number of classes\n",
      "60. Training a model on a dataset with a small number of classes\n",
      "61. Training a model on a dataset with a large number of classes\n",
      "62. Training a model on a dataset with a small number of classes\n",
      "63. Training a model on a dataset with a large number of classes\n",
      "64. Training a model on a dataset with a small number of classes\n",
      "65. Training a model on a dataset with a large number of classes\n",
      "66. Training a model on a dataset with a small number of classes\n",
      "67. Training a model on a dataset with a large number of classes\n",
      "68. Training a model on a dataset with\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Which instances can I use with Managed Spot Training in SageMaker?\"\n",
    "# prompt = PROMPT.format(context=docs, question=question)\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.01,\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"early_stopping\": True,\n",
    "        \"stop\": [\"<|endoftext|>\", \"</s>\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "def query_endpoint_with_json_payload(\n",
    "    encoded_json, \n",
    "    endpoint_name, \n",
    "    content_type=\"application/json\"\n",
    "):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, \n",
    "        ContentType=content_type, \n",
    "        Body=encoded_json\n",
    "    )\n",
    "    model_output = json.loads(response[\"Body\"].read().decode(\"utf-8\"))\n",
    "    generated_text = model_output[0][\"generated_text\"]\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "\n",
    "query_response = query_endpoint_with_json_payload(\n",
    "    encoded_json=json.dumps(payload).encode(\"utf-8\"), \n",
    "    endpoint_name=llm_endpoint_name\n",
    "    )\n",
    "\n",
    "print(prompt)\n",
    "print(query_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the generated answer is wrong or doesn't make much sense. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Improve the answer to the same question using **prompt engineering** with insightful context\n",
    "\n",
    "\n",
    "To better answer the question well, we provide extra contextual information, combine it with a prompt, and send it to model together with the question. Below is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Answer QUESTION based on CONTEXT:\\n\\nCONTEXT:{context}\\n\\nQUESTION:{question}\\n\\nANSWER:\"\"\"\n",
    "question = \"Which instances can I use with Managed Spot Training in SageMaker?\"\n",
    "context = \"\"\"Managed Spot Training can be used with all instances supported in Amazon SageMaker. Managed Spot Training is supported in all AWS Regions where Amazon SageMaker is currently available.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer QUESTION based on CONTEXT:\n",
      "\n",
      "CONTEXT:Managed Spot Training can be used with all instances supported in Amazon SageMaker. Managed Spot Training is supported in all AWS Regions where Amazon SageMaker is currently available.\n",
      "\n",
      "QUESTION:Which instances can I use with Managed Spot Training in SageMaker?\n",
      "\n",
      "ANSWER:\n",
      "\n",
      "\n",
      "You can use Managed Spot Training with all instances supported in Amazon SageMaker.\n"
     ]
    }
   ],
   "source": [
    "text_input = prompt.replace(\"{context}\", context)\n",
    "text_input = text_input.replace(\"{question}\", question)\n",
    "\n",
    "print(text_input)\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": text_input,\n",
    "    \"parameters\": {\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.01,\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"stop\": [\"<|endoftext|>\", \"</s>\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "response = query_endpoint_with_json_payload(\n",
    "    encoded_json=json.dumps(payload).encode(\"utf-8\"), \n",
    "    endpoint_name=llm_endpoint_name\n",
    "    )\n",
    "\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from step 3 tells us the chance to get the correct response significantly correlates with the insightful context you send into the LLM. \n",
    "\n",
    "**<span style=\"color:red\">Now, the question becomes where can I find the insightful context based on the user query? The answer is to use a pre-stored knowledge data base with retrieval augmented generation, as shown in step 4 below</span>.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Use RAG based approach with [LangChain](https://python.langchain.com/en/latest/index.html) and SageMaker endpoints to build a simplified question and answering application.\n",
    "\n",
    "\n",
    "We plan to use document embeddings to fetch the most relevant documents in our document knowledge library and combine them with the prompt that we provide to LLM.\n",
    "\n",
    "To achieve that, we will do following.\n",
    "\n",
    "1. **Generate embedings for each of document in the knowledge library with SageMaker GPT-J-6B FP16 embedding model.**\n",
    "2. **Identify top K most relevant documents based on user query.**\n",
    "    - 2.1 **For a query of your interest, generate the embedding of the query using the same embedding model.**\n",
    "    - 2.2 **Search the indexes of top K most relevant documents in the embedding space using in-memory Faiss search.**\n",
    "    - 2.3 **Use the indexes to retrieve the corresponded documents.**\n",
    "3. **Combine the retrieved documents with prompt and question and send them into SageMaker LLM.**\n",
    "\n",
    "\n",
    "\n",
    "Note: The retrieved document/text should be large enough to contain enough information to answer a question; but small enough to fit into the LLM prompt -- maximum sequence length of 1024 tokens. \n",
    "\n",
    "---\n",
    "To build a simiplied QA application with LangChain, we need: \n",
    "1. Wrap up our SageMaker endpoints for embedding model and LLM into `langchain.embeddings.SagemakerEndpointEmbeddings` and `langchain.llms.sagemaker_endpoint.SagemakerEndpoint`. That requires a small overwritten of `SagemakerEndpointEmbeddings` class to make it compatible with SageMaker embedding model.\n",
    "2. Prepare the dataset to build the knowledge data base. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap up our SageMaker endpoints for embedding model into `langchain.embeddings.SagemakerEndpointEmbeddings`. That requires a small overwritten of `SagemakerEndpointEmbeddings` class to make it compatible with SageMaker embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "\n",
    "\n",
    "class SagemakerEndpointEmbeddingsJumpStart(SagemakerEndpointEmbeddings):\n",
    "    def embed_documents(self, texts: List[str], chunk_size: int = 5) -> List[List[float]]:\n",
    "        \"\"\"Compute doc embeddings using a SageMaker Inference Endpoint.\n",
    "\n",
    "        Args:\n",
    "            texts: The list of texts to embed.\n",
    "            chunk_size: The chunk size defines how many input texts will\n",
    "                be grouped together as request. If None, will use the\n",
    "                chunk size specified by the class.\n",
    "\n",
    "        Returns:\n",
    "            List of embeddings, one for each text.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        _chunk_size = len(texts) if chunk_size > len(texts) else chunk_size\n",
    "\n",
    "        for i in range(0, len(texts), _chunk_size):\n",
    "            response = self._embedding_func(texts[i : i + _chunk_size])\n",
    "            print\n",
    "            results.extend(response)\n",
    "        return results\n",
    "\n",
    "\n",
    "class ContentHandler(EmbeddingsContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"text_inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        embeddings = response_json[\"embedding\"]\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "embeddings = SagemakerEndpointEmbeddingsJumpStart(\n",
    "    endpoint_name=embeddings_endpoint_name,\n",
    "    region_name=aws_region,\n",
    "    content_handler=content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we wrap up our SageMaker endpoints for LLM into `langchain.llms.sagemaker_endpoint.SagemakerEndpoint`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mSagemakerEndpoint\u001b[0m\n",
      "Params: {'endpoint_name': 'falcon-7b-instruct-bf16-endpoint', 'model_kwargs': {'do_sample': True, 'top_p': 0.9, 'temperature': 0.01, 'max_new_tokens': 1024, 'early_stopping': True, 'stop': ['<|endoftext|>', '</s>']}}\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler, SagemakerEndpoint\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.01,\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"early_stopping\": True,\n",
    "    \"stop\": [\"<|endoftext|>\", \"</s>\"]\n",
    "}\n",
    "\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "    \n",
    "    def transform_input(self, prompt: str, model_kwargs=Dict) -> bytes:\n",
    "        prompt = prompt[:1023]  #?? https://github.com/aws-samples/amazon-kendra-langchain-extensions/blob/main/kendra_retriever_samples/kendra_chat_falcon_40b.py\n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "sm_llm = SagemakerEndpoint(\n",
    "    endpoint_name=llm_endpoint_name,\n",
    "    region_name=aws_region,\n",
    "    model_kwargs=parameters,\n",
    "    content_handler=content_handler,\n",
    ")\n",
    "\n",
    "print(sm_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's download the example data and prepare it for demonstration. We will use [Amazon SageMaker FAQs](https://aws.amazon.com/sagemaker/faqs/) as knowledge library. The data are formatted in a CSV file with two columns Question and Answer. We use the Answer column as the documents of knowledge library, from which relevant documents are retrieved based on a query. \n",
    "\n",
    "**For your purpose, you can replace the example dataset of your own to build a custom question and answering application.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://jumpstart-cache-prod-us-east-2/training-datasets/Amazon_SageMaker_FAQs/Amazon_SageMaker_FAQs.csv to rag_data/Amazon_SageMaker_FAQs.csv\n"
     ]
    }
   ],
   "source": [
    "original_data = \"s3://jumpstart-cache-prod-us-east-2/training-datasets/Amazon_SageMaker_FAQs/\"\n",
    "\n",
    "!mkdir -p rag_data\n",
    "!aws s3 cp --recursive $original_data rag_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the case when you have data saved in multiple subsets. The following code will read all files that end with `.csv` and concatenate them together. Please ensure each `csv` file has the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "all_files = glob.glob(os.path.join(\"rag_data/\", \"*.csv\"))\n",
    "\n",
    "df_knowledge = pd.concat(\n",
    "    (pd.read_csv(f, header=None, names=[\"Question\", \"Answer\"]) for f in all_files),\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the `Question` column as it is not used in this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_knowledge.drop([\"Question\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amazon SageMaker is a fully managed service to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For a list of the supported Amazon SageMaker A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon SageMaker is designed for high availabi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amazon SageMaker stores code in ML storage vol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amazon SageMaker ensures that ML model artifac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Answer\n",
       "0  Amazon SageMaker is a fully managed service to...\n",
       "1  For a list of the supported Amazon SageMaker A...\n",
       "2  Amazon SageMaker is designed for high availabi...\n",
       "3  Amazon SageMaker stores code in ML storage vol...\n",
       "4  Amazon SageMaker ensures that ML model artifac..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_knowledge.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_knowledge.to_csv(\"rag_data/processed_data.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import Chroma, AtlasDB, FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.document_loaders.csv_loader import CSVLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use langchain to read the `csv` data. There are multiple built-in functions in LangChain to read different format of files such as `txt`, `html`, and `pdf`. For details, see [LangChain document loaders](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = CSVLoader(\n",
    "    file_path=\"rag_data/processed_data.csv\",\n",
    "    csv_args={\n",
    "        \"delimiter\": \",\",\n",
    "        \"quotechar\": '\"',\n",
    "    },  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents = loader.load()\n",
    "# text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
    "# texts = text_splitter.split_documents(documents) ### if you use langchain.document_loaders.TextLoader to load text file. You can uncomment the code\n",
    "## to split the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, we can build an QA application. <span style=\"color:red\">LangChain makes it extremly simple with following few lines of code</span>.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the question below, we can achieven the points in Step 4 with just a few lines of code as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Which instances can I use with Managed Spot Training in SageMaker?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_creator = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=FAISS,\n",
    "    embedding=embeddings,\n",
    "    text_splitter=CharacterTextSplitter(chunk_size=300, chunk_overlap=0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = index_creator.from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<p>Managed Spot Training in SageMaker is currently available for the following instance types:</p>\\n\\n<ul>\\n<li>m5.xlarge</li>\\n<li>c5.18xlarge</li>\\n<li>x1e12.2xlarge</li>\\n<li>x1e12.18xlarge</li>\\n<li>x1e12.24xlarge</li>\\n<li>x1e12.36xlarge</li>\\n<li>x1e12.48xlarge</li>\\n<li>x1e12.54xlarge</li>\\n<li>x1e12.72xlarge</li>\\n<li>x1e12.96xlarge</li>\\n<li>x1e12.128xlarge</li>\\n<li>x1e12.256xlarge</li>\\n<li>x1e12.512xlarge</li>\\n<li>x1e12.768xlarge</li>\\n<li>x1e12.864xlarge</li>\\n<li>x1e12.912xlarge</li>\\n<li>x1e12.924xlarge</li>\\n<li>x1e12.928xlarge</li>\\n<li>x1e12.932xlarge</li>\\n<li>x1e12.936xlarge</li>\\n<li>x1e12.952xlarge</li>\\n<li>x1e12.960xlarge</li>\\n<li>x1e12.964xlarge</li>\\n<li>x1e12.968xlarge</li>\\n<li>x1e12.980xlarge</li>\\n<li>x1e12.992xlarge</li>\\n<li>x1e12.994xlarge</li>\\n<li>x1e12.996xlarge</li>\\n<li>x1e12.998xlarge</li>\\n<li>x1e12.999xlarge</li>\\n<li>x1e12.9999xlarge</li>\\n<li>x1e12.9999xlarge</li>\\n<li>x1e12.9999xlarge</li>\\n<li>x1e12.9999xlarge</li>\\n<li>x1e12.9999xlarge</li>\\n<li>x1e12.9999xlarge</li>\\n<li>x1e12.9999xlarge</li>\\n<li>x1e12.9999xlarge</li>\\n<li>x1e12.9999xlarge</li>\\n<li>x1e12.9999xlarge</li>\\n<li>x1e12.9999xlarge</li>\\n<li>x1e12.9999xlarge</li>\\n<li>x1e12.9999xlarge</li>\\n<li>x1e12.9999xlarge</li>\\n<li>x1e12.9999xlarge</li>\\n<li>x1e12.9999xlarge</li>\\n<li>x1e12.9999xlarge</li>\\n<li>x1e12.9999xlarge</li>\\n<li>x1e12.9999xlarge</li>\\n<li>x1e12.9999xlarge</li>\\n<li>x1e12.9999xlarge</li>\\n<li>x1e12.9999xlarge</li>\\n<li>x1e12.9999xlarge</li>\\n<li>x1e12.9999xlarge</li>\\n<li>x1e12.9999xlarge</li>\\n<li>x1e12.9999xlarge</li>\\n<li>x1e12.9999xlarge</li>\\n<li>x1e12.9999xlarge</li>\\n<li>x1e12.9999xlarge</li>\\n<li>x1e12.9999xlarge</li>\\n<li>x1e12.9999xlarge</li>\\n<li>x1e12.9999xlarge</li>\\n<li>x1e12.9999xlarge</li>\\n<li>x1e12.9999xlarge</li>\\n<li>x1e12.999'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.query(question=question, llm=sm_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Customize the QA application above with different prompt.\n",
    "\n",
    "Now, we see how simple it is to use LangChain to achieve question and answering application with just few lines of code. Let's break down the above `VectorstoreIndexCreator` and see what's happening under the hood. Furthermore, we will see how to incorporate a customize prompt rather than using a default prompt with `VectorstoreIndexCreator`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we **generate embedings for each of document in the knowledge library with SageMaker GPT-J-6B FP16 embedding model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 367 ms, sys: 44.3 ms, total: 412 ms\n",
      "Wall time: 7.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "docsearch = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Which instances can I use with Managed Spot Training in SageMaker?'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the question above, we then **identify top K most relevant documents based on user query, where K = 3 in this setup**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = docsearch.similarity_search(question, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the top 3 most relevant docuemnts as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: Once a Managed Spot Training job is completed, you can see the savings in the AWS Management Console and also calculate the cost savings as the percentage difference between the duration for which the training job ran and the duration for which you were billed. Regardless of how many times your Managed Spot Training jobs are interrupted, you are charged only once for the duration for which the data was downloaded.', metadata={'source': 'rag_data/processed_data.csv', 'row': 88}),\n",
       " Document(page_content='Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: Managed Spot Training uses Amazon EC2 Spot instances for training, and these instances can be pre-empted when AWS needs capacity. As a result, Managed Spot Training jobs can run in small increments as and when capacity becomes available. The training jobs need not be restarted from scratch when there is an interruption, as Amazon SageMaker can resume the training jobs using the latest model checkpoint. The built-in frameworks and the built-in computer vision algorithms with SageMaker enable periodic checkpoints, and you can enable checkpoints with custom models.', metadata={'source': 'rag_data/processed_data.csv', 'row': 86}),\n",
       " Document(page_content='Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: If you have a consistent amount of Amazon SageMaker instance usage (measured in $/hour) and use multiple SageMaker components or expect your technology configuration (such as instance family, or Region) to change over time, SageMaker Savings Plans make it simpler to maximize your savings while providing flexibility to change the underlying technology configuration based on application needs or new innovation. The Savings Plans rate applies automatically to all eligible ML instance usage with no manual modifications required.', metadata={'source': 'rag_data/processed_data.csv', 'row': 149})]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we **combine the retrieved documents with prompt and question and send them into SageMaker LLM.** \n",
    "\n",
    "We define a customized prompt as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_template = \"\"\"Answer based on context:\\n\\n{context}\\n\\n{question}\"\"\"\n",
    "prompt_template = \"\"\"Answer QUESTION based on CONTEXT:\\n\\nCONTEXT:{context}\\n\\nQUESTION:{question}\\n\\nANSWER:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Answer QUESTION based on CONTEXT:\\n\\nCONTEXT:[Document(page_content='Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: Once a Managed Spot Training job is completed, you can see the savings in the AWS Management Console and also calculate the cost savings as the percentage difference between the duration for which the training job ran and the duration for which you were billed. Regardless of how many times your Managed Spot Training jobs are interrupted, you are charged only once for the duration for which the data was downloaded.', metadata={'source': 'rag_data/processed_data.csv', 'row': 88}), Document(page_content='Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: Managed Spot Training uses Amazon EC2 Spot instances for training, and these instances can be pre-empted when AWS needs capacity. As a result, Managed Spot Training jobs can run in small increments as and when capacity becomes available. The training jobs need not be restarted from scratch when there is an interruption, as Amazon SageMaker can resume the training jobs using the latest model checkpoint. The built-in frameworks and the built-in computer vision algorithms with SageMaker enable periodic checkpoints, and you can enable checkpoints with custom models.', metadata={'source': 'rag_data/processed_data.csv', 'row': 86}), Document(page_content='Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: If you have a consistent amount of Amazon SageMaker instance usage (measured in $/hour) and use multiple SageMaker components or expect your technology configuration (such as instance family, or Region) to change over time, SageMaker Savings Plans make it simpler to maximize your savings while providing flexibility to change the underlying technology configuration based on application needs or new innovation. The Savings Plans rate applies automatically to all eligible ML instance usage with no manual modifications required.', metadata={'source': 'rag_data/processed_data.csv', 'row': 149})]\\n\\nQUESTION:Which instances can I use with Managed Spot Training in SageMaker?\\n\\nANSWER:\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT.format(context=docs, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_chain(\n",
    "    llm=sm_llm, \n",
    "    prompt=PROMPT, \n",
    "    verbose=True, \n",
    "    chain_type=\"stuff\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Chain.prep_inputs of StuffDocumentsChain(memory=None, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7f5fc02befd0>, verbose=True, input_key='input_documents', output_key='output_text', llm_chain=LLMChain(memory=None, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7f5fc02befd0>, verbose=True, prompt=PromptTemplate(input_variables=['context', 'question'], output_parser=None, partial_variables={}, template='Answer QUESTION based on CONTEXT:\\n\\nCONTEXT:{context}\\n\\nQUESTION:{question}\\n\\nANSWER:', template_format='f-string', validate_template=True), llm=SagemakerEndpoint(cache=None, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7f5fc02befd0>, client=<botocore.client.SageMakerRuntime object at 0x7f5f8a5fe220>, endpoint_name='falcon-7b-instruct-bf16-endpoint', region_name='us-east-1', credentials_profile_name=None, content_handler=<__main__.ContentHandler object at 0x7f5f8ac36fa0>, model_kwargs={'do_sample': True, 'top_p': 0.9, 'temperature': 0.01, 'max_new_tokens': 1024, 'early_stopping': True, 'stop': ['<|endoftext|>', '</s>']}, endpoint_kwargs=None), output_key='text'), document_prompt=PromptTemplate(input_variables=['page_content'], output_parser=None, partial_variables={}, template='{page_content}', template_format='f-string', validate_template=True), document_variable_name='context')>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.prep_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send the top 3 most relevant docuemnts and question into LLM to get a answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAnswer QUESTION based on CONTEXT:\n",
      "\n",
      "CONTEXT:Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: Once a Managed Spot Training job is completed, you can see the savings in the AWS Management Console and also calculate the cost savings as the percentage difference between the duration for which the training job ran and the duration for which you were billed. Regardless of how many times your Managed Spot Training jobs are interrupted, you are charged only once for the duration for which the data was downloaded.\n",
      "\n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: Managed Spot Training uses Amazon EC2 Spot instances for training, and these instances can be pre-empted when AWS needs capacity. As a result, Managed Spot Training jobs can run in small increments as and when capacity becomes available. The training jobs need not be restarted from scratch when there is an interruption, as Amazon SageMaker can resume the training jobs using the latest model checkpoint. The built-in frameworks and the built-in computer vision algorithms with SageMaker enable periodic checkpoints, and you can enable checkpoints with custom models.\n",
      "\n",
      "Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: If you have a consistent amount of Amazon SageMaker instance usage (measured in $/hour) and use multiple SageMaker components or expect your technology configuration (such as instance family, or Region) to change over time, SageMaker Savings Plans make it simpler to maximize your savings while providing flexibility to change the underlying technology configuration based on application needs or new innovation. The Savings Plans rate applies automatically to all eligible ML instance usage with no manual modifications required.\n",
      "\n",
      "QUESTION:Which instances can I use with Managed Spot Training in SageMaker?\n",
      "\n",
      "ANSWER:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = chain(\n",
    "    {\"input_documents\": docs, \"question\": question}, \n",
    "    return_only_outputs=True)[\"output_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the final answer from LLM as below, which is accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nYou can use the following instances with Managed Spot Training in SageMaker:\\n\\n1. Amazon EC2 Spot instances\\n2. Amazon EC2 Reserved instances\\n3. Amazon EC2 On-Demand instances\\n4. Amazon EC2 Auto Scaling instances\\n5. Amazon EC2 Batch instances\\n6. Amazon EC2 Spot instances with a minimum bid price of $0.50 per hour\\n7. Amazon EC2 Spot instances with a minimum bid price of $0.25 per hour\\n\\nNote:\\n\\n1. Managed Spot Training is only available for Amazon SageMaker training jobs.\\n2. Managed Spot Training is only available for Amazon SageMaker training jobs that are using Amazon EC2 Spot instances.\\n3. Managed Spot Training is only available for Amazon SageMaker training jobs that are using Amazon EC2 Spot instances with a minimum bid price of $0.50 per hour.\\n4. Managed Spot Training is only available for Amazon SageMaker training jobs that are using Amazon EC2 Spot instances with a minimum bid price of $0.25 per hour.\\n\\nFor more information on Managed Spot Training, please refer to the following resources:\\n\\n1. Amazon SageMaker Managed Spot Training:\\n2. Amazon SageMaker Spot Instance Pricing:\\n3. Amazon SageMaker Spot Instance Savings Plans:'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer QUESTION based on CONTEXT:\n",
      "\n",
      "CONTEXT:[Document(page_content='Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: Once a Managed Spot Training job is completed, you can see the savings in the AWS Management Console and also calculate the cost savings as the percentage difference between the duration for which the training job ran and the duration for which you were billed. Regardless of how many times your Managed Spot Training jobs are interrupted, you are charged only once for the duration for which the data was downloaded.', metadata={'source': 'rag_data/processed_data.csv', 'row': 88}), Document(page_content='Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: Managed Spot Training uses Amazon EC2 Spot instances for training, and these instances can be pre-empted when AWS needs capacity. As a result, Managed Spot Training jobs can run in small increments as and when capacity becomes available. The training jobs need not be restarted from scratch when there is an interruption, as Amazon SageMaker can resume the training jobs using the latest model checkpoint. The built-in frameworks and the built-in computer vision algorithms with SageMaker enable periodic checkpoints, and you can enable checkpoints with custom models.', metadata={'source': 'rag_data/processed_data.csv', 'row': 86}), Document(page_content='Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.: If you have a consistent amount of Amazon SageMaker instance usage (measured in $/hour) and use multiple SageMaker components or expect your technology configuration (such as instance family, or Region) to change over time, SageMaker Savings Plans make it simpler to maximize your savings while providing flexibility to change the underlying technology configuration based on application needs or new innovation. The Savings Plans rate applies automatically to all eligible ML instance usage with no manual modifications required.', metadata={'source': 'rag_data/processed_data.csv', 'row': 149})]\n",
      "\n",
      "QUESTION:Which instances can I use with Managed Spot Training in SageMaker?\n",
      "\n",
      "ANSWER:\n",
      " You can use Amazon EC2 Spot instances with Managed Spot Training in SageMaker.\n"
     ]
    }
   ],
   "source": [
    "prompt = PROMPT.format(context=docs, question=question)\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.01,\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"early_stopping\": True,\n",
    "        \"stop\": [\"<|endoftext|>\", \"</s>\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "query_response = query_endpoint_with_json_payload(\n",
    "    encoded_json=json.dumps(payload).encode(\"utf-8\"), \n",
    "    endpoint_name=llm_endpoint_name\n",
    "    )\n",
    "\n",
    "print(prompt)\n",
    "print(query_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Clean up the environment\n",
    "Delete the endpoints if you don't use them because they incure cost per hour!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting endpoint: falcon-7b-instruct-bf16-endpoint\n",
      "Deleting endpoint: gpt-j-6b-endpoint\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'fcceb0c2-0285-4757-852b-1bbe4c625c09',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'fcceb0c2-0285-4757-852b-1bbe4c625c09',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Thu, 24 Aug 2023 03:30:53 GMT'},\n",
       "  'RetryAttempts': 1}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a low-level SageMaker service client.\n",
    "sagemaker_client = boto3.client('sagemaker', region_name=aws_region)\n",
    "\n",
    "# Delete endpoint\n",
    "print('Deleting endpoint:', llm_endpoint_name)\n",
    "sagemaker_client.delete_endpoint(EndpointName=llm_endpoint_name)\n",
    "sagemaker_client.delete_endpoint_config(EndpointConfigName=llm_endpoint_name)\n",
    "\n",
    "# Delete endpoint\n",
    "print('Deleting endpoint:', embeddings_endpoint_name)\n",
    "sagemaker_client.delete_endpoint(EndpointName=embeddings_endpoint_name)\n",
    "sagemaker_client.delete_endpoint_config(EndpointConfigName=embeddings_endpoint_name)\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
